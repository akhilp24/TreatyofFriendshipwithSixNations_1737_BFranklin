A Treaty of friendship held with the chiefs of the Six Nations, at Philadelphia, in September and October, 1736. Philadelphia: Printed and sold by B. Franklin, at the new printing-office near the market, 1737. 14 pages; 31 cm (folio). English.

University of Pennsylvania, Kislak Center for Special Collections, Rare Books and Manuscripts, Curtis Collection, Folio 112F.

Catalogue entry: https://find.library.upenn.edu/catalog/9929453563503681?hld_id=resource_link_0
Colenda entry: https://colenda.library.upenn.edu/catalog/81431-p32806128

The metadata categories I selected were selected to prioritize the diplomatic and cultural significance of the treaty document while addressing the practical challenges of digitizing historical materials. Instead of focusing solely on the technical aspects, I wanted to include and account for speaker representation through fields like "colonial_officials" and "indigenous_nations_mentioned," since it is a document that records a formal negotiation between sovereign nations. I included "page_type" and "content_description" to handle the friction of digitizing and capturing not just what page number appears but also what type of content each image has and how it functions within the broader treaty structure. I also added fields like "text_legibility" and "notable_physical_features" to document what was lost or gained in the digitization process, while the "historical_significance_notes" column has space for contextual analysis. 



Cleaning and Extracting Text

For my text extraction process, I used Tesseract's command line interface to analyze all of the images, following the sample commands provided by Professor Trettien. The OCR results were a bit mixed: on clear pages, the extraction was pretty accurate, but I noticed there was frequent misrecognition of the letter "s" as "f," which is how the text appears given this is from the 18th-century. This was a consistent issue throughout most of the documents as well as not capturing the quotation marks in the correct space. To improve accuracy, I experimented by running the OCR output through ChatGPT using a prompt specifically mentioning the historical "s" & "f" confusion, which resulted in better corrections because I indicated to the model to determine which character made sense for each word. The "s" letter was also being indicated incorrectly as "{". Also, oftentimes, there many issues in capturing the appropriate capitalization or line breaks within the content with Tesseract; however, ChatGPT did a much better job. After the OCR steps, I needed to do a fair amount of manual correction by reading line by line and comparing to the images, especially for headings, margins, or unusually formatted sections. Fortunately, there were no tables or embedded images in the source material, so I didn't need to develop a workflow for extracting that type of media. One limitation of the plain text format was not being able to preserve formatting such as italics or bold, which would be possible if converted to markdown or a rich text format, but I kept the transcription strictly in plain text. There were also certain things I could not capture given the structure of the text file, such as text structure in columns, such as an instance where there was a list of speakers separated into three columns.
